{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import norm, skew, boxcox_normmax\nimport warnings\nfrom sklearn import decomposition\nfrom scipy.special import boxcox1p\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-01T07:01:13.694208Z","iopub.execute_input":"2022-07-01T07:01:13.694682Z","iopub.status.idle":"2022-07-01T07:01:15.142708Z","shell.execute_reply.started":"2022-07-01T07:01:13.694572Z","shell.execute_reply":"2022-07-01T07:01:15.141581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:01:15.144652Z","iopub.execute_input":"2022-07-01T07:01:15.144961Z","iopub.status.idle":"2022-07-01T07:01:15.150708Z","shell.execute_reply.started":"2022-07-01T07:01:15.144925Z","shell.execute_reply":"2022-07-01T07:01:15.149602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\ntrain = pd.read_csv('../input/moviealldata/movietrain5.csv')\ntest = pd.read_csv('../input/moviealldata/movietest5.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:01:15.152069Z","iopub.execute_input":"2022-07-01T07:01:15.152478Z","iopub.status.idle":"2022-07-01T07:01:15.205590Z","shell.execute_reply.started":"2022-07-01T07:01:15.152438Z","shell.execute_reply":"2022-07-01T07:01:15.204665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\ntrain_ID = train.pop('id')\ntest_ID = test.pop('id')","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:01:15.207708Z","iopub.execute_input":"2022-07-01T07:01:15.208358Z","iopub.status.idle":"2022-07-01T07:01:15.225473Z","shell.execute_reply.started":"2022-07-01T07:01:15.208314Z","shell.execute_reply":"2022-07-01T07:01:15.224065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"Total score\"] = np.log1p(train[\"Total score\"])","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:01:15.227360Z","iopub.execute_input":"2022-07-01T07:01:15.228106Z","iopub.status.idle":"2022-07-01T07:01:15.239014Z","shell.execute_reply.started":"2022-07-01T07:01:15.228042Z","shell.execute_reply":"2022-07-01T07:01:15.238209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train['Total score'].values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['Total score'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:01:15.240564Z","iopub.execute_input":"2022-07-01T07:01:15.241091Z","iopub.status.idle":"2022-07-01T07:01:15.263719Z","shell.execute_reply.started":"2022-07-01T07:01:15.241046Z","shell.execute_reply":"2022-07-01T07:01:15.262630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#one hot 方法\ndef generate_one_hot(df,sentence_col):\n   temp=[]\n   temp=np.array(pd.get_dummies(df[sentence_col]))\n   #print(temp)\n   for i in range(temp[0].size):\n          df[sentence_col+'_{}'.format(i)]=temp[:,i]\n    \n   return df\n\ngenerate_one_hot(all_data,\"Motion Picture Rating(MPAA)\")\nall_data=all_data.drop(\"Motion Picture Rating(MPAA)\",axis=1)\n\n#generate_one_hot(X_train,\"countries of origina\")\nall_data=all_data.drop(\"countries of origina\",axis=1)\n\n#generate_one_hot(X_train,\"language\")\nall_data=all_data.drop(\"language\",axis=1)\n\ngenerate_one_hot(all_data,\"movie series\")\nall_data=all_data.drop(\"movie series\",axis=1)\nall_data=all_data.drop(\"stars\",axis=1)\nall_data=all_data.drop(\"film\",axis=1)\nall_data=all_data.drop(\"time\",axis=1)\nall_data=all_data.drop(\"Production Companies\",axis=1)\nall_data=all_data.drop(\"Script\",axis=1)\nall_data=all_data.drop(\"Directors\",axis=1)    \nall_data=all_data.drop(\"writers\",axis=1)\nall_data=all_data.drop(\"Filming locations\",axis=1)\n\nall_data=all_data.drop(\"Grnres\",axis=1)\n#train_df=train_df.drop(\"score\",axis=1)\nall_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:01:15.265204Z","iopub.execute_input":"2022-07-01T07:01:15.265462Z","iopub.status.idle":"2022-07-01T07:01:15.326965Z","shell.execute_reply.started":"2022-07-01T07:01:15.265430Z","shell.execute_reply":"2022-07-01T07:01:15.325964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all_data['year']=all_data['year'].fillna('0') \nall_data=all_data.fillna('0') \n#np.isnan(all_data)\n#np.where(np.isnan(all_data))\n#all_data['year'].isnull()\nall_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:01:15.328821Z","iopub.execute_input":"2022-07-01T07:01:15.329072Z","iopub.status.idle":"2022-07-01T07:01:15.357242Z","shell.execute_reply.started":"2022-07-01T07:01:15.329042Z","shell.execute_reply":"2022-07-01T07:01:15.356028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nall_data = pd.get_dummies(all_data)\nprint(all_data.shape)\n\npca = decomposition.PCA()\n\ntrain = all_data[:ntrain].values\ntest = all_data[ntrain:].values","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:01:15.358706Z","iopub.execute_input":"2022-07-01T07:01:15.358949Z","iopub.status.idle":"2022-07-01T07:01:15.389738Z","shell.execute_reply.started":"2022-07-01T07:01:15.358921Z","shell.execute_reply":"2022-07-01T07:01:15.388974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.impute import SimpleImputer\n#my_imputer = SimpleImputer()\n#data_with_imputed_values = my_imputer.fit_transform(train)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:01:15.391813Z","iopub.execute_input":"2022-07-01T07:01:15.392763Z","iopub.status.idle":"2022-07-01T07:01:15.396951Z","shell.execute_reply.started":"2022-07-01T07:01:15.392718Z","shell.execute_reply":"2022-07-01T07:01:15.395771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model begin ====================================================================== #\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\n\nn_folds = 5\n\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train)\n    mse = np.sqrt(-cross_val_score(model, train, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n    return mse\n\n\nkfolds = KFold(n_splits=n_folds, shuffle=True, random_state=42)\nalph = [0.01, 0.001, 0.0001, 0.0002, 0.0004, 0.0008, 0.002, 0.004, 0.008, 1, 2, 4, 6, 8, 10, 12]\nalph2 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\nlasso = make_pipeline(RobustScaler(), LassoCV(alphas=alph, cv=kfolds, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNetCV(alphas=alph, l1_ratio=.9, cv=kfolds, random_state=3))\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alph2, cv=kfolds))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10,\n                                   loss='huber', random_state=5)\n\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468,\n                             learning_rate=0.05, max_depth=3,\n                             min_child_weight=2, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=True,\n                             random_state=7, nthread=-1)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression', num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin=55, bagging_fraction=0.8,\n                              bagging_freq=5, feature_fraction=0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf=6, min_sum_hessian_in_leaf=11)\n\nstacked_averaged_models = StackingCVRegressor(regressors=(ENet, GBoost, KRR),\n                                              meta_regressor=lasso,\n                                              use_features_in_secondary=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:01:15.398801Z","iopub.execute_input":"2022-07-01T07:01:15.399350Z","iopub.status.idle":"2022-07-01T07:01:16.692538Z","shell.execute_reply.started":"2022-07-01T07:01:15.399302Z","shell.execute_reply":"2022-07-01T07:01:16.691554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:01:16.694980Z","iopub.execute_input":"2022-07-01T07:01:16.695252Z","iopub.status.idle":"2022-07-01T07:01:16.700477Z","shell.execute_reply.started":"2022-07-01T07:01:16.695219Z","shell.execute_reply":"2022-07-01T07:01:16.699289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 下一步，更改融合方式\n\nmodel_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_importance = model_xgb.feature_importances_\nxgb_out = np.argsort(xgb_importance)\n\nprint(rmsle(y_train, xgb_train_pred))\nmodel_lgb.fit(train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:01:16.701929Z","iopub.execute_input":"2022-07-01T07:01:16.702320Z","iopub.status.idle":"2022-07-01T07:01:28.589192Z","shell.execute_reply.started":"2022-07-01T07:01:16.702282Z","shell.execute_reply":"2022-07-01T07:01:28.588196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"booster = model_lgb.booster_\nlgb_importance = booster.feature_importance(importance_type='split')\nlgb_out = np.argsort(lgb_importance)\n\nlgb_train_pred = model_lgb.predict(train)\nprint(rmsle(y_train, lgb_train_pred))\nprint('RMSLE score on train data:')\n# lasso.fit(train, y_train)\n# lasso_train_pred = lasso.predict(train)\n# lasso_pred = np.expm1(lasso.predict(test))\n\nGBoost.fit(train, y_train)\nGBoost_train_pred = GBoost.predict(train)\nGBT_feature = GBoost.feature_importances_\ngbt_out = np.argsort(GBT_feature)\ndrop_num = 50\nlgb_out = lgb_out[:drop_num]\nxgb_out = xgb_out[:drop_num]\ngbt_out = gbt_out[:drop_num]\n# drop_feature = [val for val in lgb_out if (val in xgb_out and val in gbt_out)]\ndrop_feature = list(set(lgb_out).union(xgb_out).union(gbt_out))\nprint(drop_feature)\ntrain = np.delete(train, drop_feature, axis=1)\ntest = np.delete(test, drop_feature, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:01:28.593677Z","iopub.execute_input":"2022-07-01T07:01:28.594861Z","iopub.status.idle":"2022-07-01T07:01:38.049584Z","shell.execute_reply.started":"2022-07-01T07:01:28.594802Z","shell.execute_reply":"2022-07-01T07:01:38.048608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ========================================== pred ===================================#\nstacked_averaged_models.fit(train, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test))\nprint(rmsle(y_train, stacked_train_pred))\n\nmodel_lgb.fit(train, y_train)\nmodel_xgb.fit(train, y_train)\nxgb_pred = np.expm1(model_xgb.predict(test))\n# GBoost_pred = np.expm1(GBoost.predict(test))\nlgb_pred = np.expm1(model_lgb.predict(test))\nprint(rmsle(y_train, stacked_train_pred * 0.7 + xgb_train_pred * 0.15 + lgb_train_pred * 0.15))\n#ensemble = stacked_pred * 0.7 + xgb_pred * 0.15 + lgb_pred * 0.15\nensemble = (stacked_pred + xgb_pred + lgb_pred)/3\nsub_LGB = pd.DataFrame()\nsub_LGB['id'] = test_ID\nsub_LGB['Total score'] = lgb_pred\nsub_XGB = pd.DataFrame()\nsub_XGB['id'] = test_ID\nsub_XGB['Total score'] = xgb_pred\nsub_LASSO = pd.DataFrame()\nsub_LASSO['id'] = test_ID\nsub_LASSO['Total score'] = stacked_pred\nsub_ensemble = pd.DataFrame()\nsub_ensemble['id'] = test_ID\nsub_ensemble['Total score'] = stacked_pred\n#q1 = submission['Total score'].quantile(0.005)\n#q2 = submission['Total score'].quantile(0.995)\n#submission['Total score'] = submission['Total score'].apply(lambda x: x if x > q1 else x * 0.85)\n#submission['Total score'] = submission['Total score'].apply(lambda x: x if x < q2 else x * 1.1)\nsub_LGB.to_csv('./sub_LGB.csv', index=False)\nsub_XGB.to_csv('./sub_XGB.csv', index=False)\nsub_LASSO.to_csv('./sub_LASSO.csv', index=False)\nsub_ensemble.to_csv('./sub_ensemble.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:01:38.051102Z","iopub.execute_input":"2022-07-01T07:01:38.051736Z","iopub.status.idle":"2022-07-01T07:02:47.459433Z","shell.execute_reply.started":"2022-07-01T07:01:38.051698Z","shell.execute_reply":"2022-07-01T07:02:47.458714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\n#from sklearn.learning_curve import learning_curve\nfrom sklearn.datasets import load_digits # 导入手写数字集\nfrom sklearn.model_selection import learning_curve\n#绘制学习曲线，以确定模型的状况\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        train_sizes=np.linspace(.1, 1.0, 5)):\n#     \"\"\"\n#     画出data在某模型上的learning curve.\n#     参数解释\n#     ----------\n#     estimator : 你用的分类器。\n#     title : 表格的标题。\n#     X : 输入的feature，numpy类型\n#     y : 输入的target vector\n#     ylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点\n#     cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为3份)\n#     \"\"\"\n    plt.figure()\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=5, n_jobs=1, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    plt.legend(loc=\"best\")\n    plt.grid(\"on\") \n    if ylim:\n        plt.ylim(ylim)\n    plt.title(title)\n    plt.show()\n#少样本的情况情况下绘出学习曲线\nplot_learning_curve(LinearSVC(C=10.0), \"LinearSVC(C=10.0)\",\n                    train, y_train, ylim=(0.1, 1.01),\n                    train_sizes=np.linspace(.1, 1, 5))","metadata":{"execution":{"iopub.status.busy":"2022-07-01T07:02:47.460827Z","iopub.execute_input":"2022-07-01T07:02:47.461339Z","iopub.status.idle":"2022-07-01T07:02:47.830006Z","shell.execute_reply.started":"2022-07-01T07:02:47.461295Z","shell.execute_reply":"2022-07-01T07:02:47.828989Z"},"trusted":true},"execution_count":null,"outputs":[]}]}